#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”„å¬›ä¼ è®­ç»ƒæ•°æ®é¢„å¤„ç†è„šæœ¬

å°†åŸå§‹JSONæ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„JSONLæ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python training/huanhuan_data_prepare.py          # å¤„ç†å…¨éƒ¨æ•°æ®
    python training/huanhuan_data_prepare.py 100     # å¤„ç†100æ¡æ•°æ®
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from loguru import logger
import random

class HuanHuanDataProcessor:
    """
    ç”„å¬›ä¼ æ•°æ®å¤„ç†å™¨
    """
    
    def __init__(self, max_samples: Optional[int] = None):
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆdataScriptsçš„ä¸Šçº§ç›®å½•ï¼‰
        script_dir = Path(__file__).parent
        project_root = script_dir.parent
        
        # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºç›®å½•çš„ç›¸å¯¹è·¯å¾„
        self.input_dir = project_root / "data" / "raw"
        self.output_dir = project_root / "data"
        self.max_samples = max_samples
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.input_dir.exists():
            logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
            logger.info("è¯·å…ˆè¿è¡Œ: python scripts/download_data.py")
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_json_data(self) -> List[Dict]:
        """
        åŠ è½½JSONè®­ç»ƒæ•°æ®
        """
        all_data = []
        
        # æŸ¥æ‰¾huanhuan.jsonæ–‡ä»¶
        json_file = self.input_dir / "huanhuan.json"
        if json_file.exists():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_data.extend(data)
                        logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {len(data)} æ¡")
                    else:
                        logger.warning(f"æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {json_file.name}")
            except Exception as e:
                logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        else:
            logger.error(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {json_file}")
        
        return all_data
    
    def process_data(self) -> List[Dict]:
        """
        å¤„ç†è®­ç»ƒæ•°æ®
        """
        logger.info("å¼€å§‹å¤„ç†ç”„å¬›ä¼ æ•°æ®...")
        
        # åŠ è½½JSONæ•°æ®
        data = self.load_json_data()
        
        if not data:
            logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
            return []
        
        # éªŒè¯æ•°æ®æ ¼å¼
        valid_data = []
        for item in data:
            if isinstance(item, dict) and all(key in item for key in ['instruction', 'input', 'output']):
                valid_data.append({
                    "instruction": item['instruction'],
                    "input": item['input'],
                    "output": item['output']
                })
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œåˆ™é™åˆ¶æ•°æ®é‡
        if self.max_samples and self.max_samples < len(valid_data):
            # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡çš„æ•°æ®
            random.shuffle(valid_data)
            valid_data = valid_data[:self.max_samples]
            logger.info(f"é™åˆ¶æ•°æ®é‡ä¸º: {self.max_samples} æ¡")
        
        logger.info(f"å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®: {len(valid_data)} æ¡")
        return valid_data
    
    def split_data(self, data: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        """
        random.shuffle(data)
        
        total_size = len(data)
        train_size = int(total_size * 0.8)
        val_size = int(total_size * 0.1)
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size + val_size]
        test_data = data[train_size + val_size:]
        
        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒé›†: {len(train_data)}, éªŒè¯é›†: {len(val_data)}, æµ‹è¯•é›†: {len(test_data)}")
        
        return train_data, val_data, test_data
    
    def save_data(self, train_data: List[Dict], val_data: List[Dict], test_data: List[Dict]):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®ä¸ºJSONLæ ¼å¼
        """
        # åˆ›å»ºprocessedå­ç›®å½•ç”¨äºå­˜æ”¾å¤„ç†åçš„æ•°æ®
        processed_dir = self.output_dir / "processed"
        processed_dir.mkdir(parents=True, exist_ok=True)
        
        datasets = {
            "train": train_data,
            "validation": val_data,
            "test": test_data
        }
        
        for split_name, split_data in datasets.items():
            jsonl_file = processed_dir / f"{split_name}.jsonl"
            with open(jsonl_file, 'w', encoding='utf-8') as f:
                for item in split_data:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            logger.info(f"ä¿å­˜ {split_name}.jsonl: {len(split_data)} æ¡æ•°æ®")
    
    def run(self) -> bool:
        """
        æ‰§è¡Œæ•°æ®é¢„å¤„ç†
        """
        try:
            # å¤„ç†æ•°æ®
            data = self.process_data()
            
            if not data:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
                return False
            
            # åˆ†å‰²æ•°æ®
            train_data, val_data, test_data = self.split_data(data)
            
            # ä¿å­˜æ•°æ®
            self.save_data(train_data, val_data, test_data)
            
            logger.info("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
            logger.info("ğŸ“ æ¥ä¸‹æ¥å¯ä»¥è¿è¡Œ: python training/huanhuan_train.py")
            
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="ç”„å¬›ä¼ è®­ç»ƒæ•°æ®é¢„å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python training/huanhuan_data_prepare.py          # å¤„ç†å…¨éƒ¨æ•°æ®
  python training/huanhuan_data_prepare.py 100     # å¤„ç†100æ¡æ•°æ®
        """
    )
    
    # ä½ç½®å‚æ•°ï¼šæ•°æ®é‡
    parser.add_argument(
        'data_count', 
        nargs='?', 
        type=int, 
        help='è¦å¤„ç†çš„æ•°æ®æ¡æ•°ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™å¤„ç†å…¨éƒ¨æ•°æ®ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è·å–æœ€å¤§æ ·æœ¬æ•°
    max_samples = args.data_count
    
    if max_samples is not None:
        if max_samples <= 0:
            logger.error("æ•°æ®é‡å¿…é¡»å¤§äº0")
            exit(1)
        logger.info(f"å°†å¤„ç†æœ€å¤š {max_samples} æ¡æ•°æ®")
    else:
        logger.info("å°†å¤„ç†å…¨éƒ¨æ•°æ®")
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡Œ
    try:
        processor = HuanHuanDataProcessor(max_samples=max_samples)
        if not processor.run():
            exit(1)
    except FileNotFoundError:
        exit(1)

if __name__ == "__main__":
    main()