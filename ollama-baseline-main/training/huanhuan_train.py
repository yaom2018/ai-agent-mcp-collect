#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”„å¬›è§’è‰²æ¨¡å‹è®­ç»ƒè„šæœ¬

åŸºäºã€Šç”„å¬›ä¼ ã€‹è§’è‰²æ•°æ®è¿›è¡ŒLoRAå¾®è°ƒï¼Œ
è®­ç»ƒå‡ºå…·æœ‰ç”„å¬›è¯­è¨€é£æ ¼çš„å¯¹è¯æ¨¡å‹ã€‚

å‚è€ƒé¡¹ç›®: https://github.com/KMnO4-zx/huanhuan-chat
ä½¿ç”¨æ–¹æ³•:
    python training/huanhuan_train.py
"""

import os
import sys
import json
import yaml
from pathlib import Path
from typing import Dict, List, Optional
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from loguru import logger

# æ­£ç¡®é…ç½®tokenizerså¹¶è¡ŒåŒ–
# åœ¨è®­ç»ƒå¼€å§‹å‰è®¾ç½®ï¼Œé¿å…forkåçš„å¹¶è¡ŒåŒ–å†²çª
if "TOKENIZERS_PARALLELISM" not in os.environ:
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®logger
logger.remove()  # ç§»é™¤é»˜è®¤çš„æ§åˆ¶å°è¾“å‡º
logger.add(sys.stdout, level="INFO")  # æ·»åŠ æ§åˆ¶å°è¾“å‡º
script_dir = Path(__file__).parent
log_dir = script_dir / "logs"
log_dir.mkdir(exist_ok=True)

logger.add(str(log_dir / "training.log"), rotation="10 MB", retention="7 days", level="INFO")



class HuanHuanDataset(Dataset):
    """
    ç”„å¬›è§’è‰²å¯¹è¯æ•°æ®é›†
    """
    
    def __init__(self, data_file: str, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.conversations = self.load_conversations(data_file)
        
        # è®¾ç½®ç‰¹æ®Štoken
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    
    def load_conversations(self, data_file: str) -> List[Dict]:
        """
        åŠ è½½å¯¹è¯æ•°æ®
        """
        conversations = []
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        conversations.append(data)
            logger.info(f"åŠ è½½äº† {len(conversations)} æ¡å¯¹è¯æ•°æ®")
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
        
        return conversations
    
    def __len__(self):
        return len(self.conversations)
    
    def __getitem__(self, idx):
        conversation = self.conversations[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        instruction = conversation.get('instruction', '')
        input_text = conversation.get('input', '')
        output = conversation.get('output', '')
        
        # æ ¼å¼åŒ–å¯¹è¯
        if input_text:
            prompt = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nå›åº”ï¼š"
        else:
            prompt = f"æŒ‡ä»¤ï¼š{instruction}\nå›åº”ï¼š"
        
        # å®Œæ•´æ–‡æœ¬
        full_text = prompt + output + self.tokenizer.eos_token
        
        # ç¼–ç 
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        # è®¡ç®—æ ‡ç­¾ï¼ˆåªå¯¹å›åº”éƒ¨åˆ†è®¡ç®—æŸå¤±ï¼‰
        prompt_encoding = self.tokenizer(
            prompt,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors='pt'
        )
        
        labels = encoding['input_ids'].clone()
        prompt_length = len(prompt_encoding['input_ids'][0])
        labels[0, :prompt_length] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„æŸå¤±
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': labels.flatten()
        }

class HuanHuanTrainer:
    """
    ç”„å¬›è§’è‰²æ¨¡å‹è®­ç»ƒå™¨
    """
    
    def __init__(self, config_path: str = "./huanhuan_config_fast.yaml"):
        self.config_path = config_path
        self.config = None
        self.device = None
        self.model = None
        self.tokenizer = None
        self.train_dataset = None
        self.eval_dataset = None
        self.trainer = None
        
        logger.info(f"HuanHuanTrainer åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®æ–‡ä»¶: {config_path}")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.load_config()
        self.setup_device()
        self.load_model_and_tokenizer()
        self.setup_lora()
        self.setup_training_arguments()
    
    def load_config(self):
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        """
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.config['training']['output_dir'], exist_ok=True)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def setup_device(self):
        """
        è®¾ç½®è®¡ç®—è®¾å¤‡
        """
        device_config = self.config['system']['device']
        
        if device_config == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                logger.info(f"ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = torch.device('mps')
                logger.info("ä½¿ç”¨Apple Silicon MPSè®¾å¤‡")
            else:
                self.device = torch.device('cpu')
                logger.info("ä½¿ç”¨CPUè®¾å¤‡")
        else:
            self.device = torch.device(device_config)
            logger.info(f"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {self.device}")
    
    def load_model_and_tokenizer(self):
        """
        åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        """
        model_name = self.config['model']['base_model']
        
        logger.info(f"åŠ è½½åˆ†è¯å™¨: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side='left'
        )
        
        # è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float32,  # å¼ºåˆ¶ä½¿ç”¨float32
            device_map=None  # ç¦ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        )
        
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = self.model.to(self.device)
        logger.info(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {self.model.num_parameters():,}")
    
    def setup_lora(self):
        """
        è®¾ç½®LoRAé…ç½®
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨LoRAï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        if not self.config['lora'].get('enabled', True):
            logger.info("æœªå¯ç”¨LoRAï¼Œä½¿ç”¨å…¨å‚æ•°å¾®è°ƒ")
            return
        
        logger.info("é…ç½®LoRAå‚æ•°")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.config['lora'].get('r', 8),
            lora_alpha=self.config['lora'].get('lora_alpha', 16),
            lora_dropout=self.config['lora'].get('lora_dropout', 0.1),
            target_modules=self.config['lora'].get('target_modules', ["q_proj", "v_proj"]),
            bias=self.config['lora'].get('bias', 'none')
        )
        
        self.model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        logger.info(f"æ€»å‚æ•°é‡: {total_params:,}")
    
    def prepare_datasets(self):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›†
        """
        # åˆ†åˆ«åŠ è½½ä¸‰ä¸ªæ–‡ä»¶
        train_dataset = HuanHuanDataset(
            data_file=self.config['data']['train_file'],
            tokenizer=self.tokenizer,
            max_length=self.config['model'].get('max_length', 2048)
        )
        
        val_dataset = HuanHuanDataset(
            data_file=self.config['data']['validation_file'],
            tokenizer=self.tokenizer,
            max_length=self.config['model'].get('max_length', 2048)
        )
        
        # æµ‹è¯•é›†åœ¨è®­ç»ƒæ—¶ä¸ç”¨ï¼Œä½†è¦ä¿ç•™ç”¨äºæœ€ç»ˆè¯„ä¼°
        test_dataset = HuanHuanDataset(
            data_file=self.config['data']['test_file'],
            tokenizer=self.tokenizer,
            max_length=self.config['model'].get('max_length', 2048)
        )
        
        logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        logger.info(f"æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_dataset, val_dataset, test_dataset
    def setup_training_arguments(self):
        """
        è®¾ç½®è®­ç»ƒå‚æ•°å¹¶åˆ›å»ºTrainer
        """
        training_config = self.config['training']
        
        # æ ¹æ®è®¾å¤‡ç±»å‹åŠ¨æ€è°ƒæ•´å‚æ•°
        is_mps_device = self.device.type == 'mps'
        is_cuda_device = self.device.type == 'cuda'
        
        # æ•°æ®åŠ è½½å™¨é…ç½®ï¼šæ ¹æ®è®¾å¤‡å’Œç³»ç»Ÿèƒ½åŠ›è°ƒæ•´
        if is_mps_device:
            # MPSè®¾å¤‡ä¼˜åŒ–é…ç½®
            dataloader_num_workers = 0  # MPSè®¾å¤‡å»ºè®®ä½¿ç”¨å•çº¿ç¨‹
            dataloader_pin_memory = False  # MPSä¸æ”¯æŒpin_memory
            logger.info("æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½¿ç”¨MPSä¼˜åŒ–é…ç½®")
        elif is_cuda_device:
            # CUDAè®¾å¤‡å¯ä»¥ä½¿ç”¨æ›´å¤šä¼˜åŒ–
            dataloader_num_workers = min(4, os.cpu_count() or 1)
            dataloader_pin_memory = True
            logger.info(f"æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨{dataloader_num_workers}ä¸ªæ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹")
        else:
            # CPUè®¾å¤‡é…ç½®
            dataloader_num_workers = min(2, os.cpu_count() or 1)
            dataloader_pin_memory = False
            logger.info(f"ä½¿ç”¨CPUè®¾å¤‡ï¼Œé…ç½®{dataloader_num_workers}ä¸ªæ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹")
        
        training_args = TrainingArguments(
            output_dir=training_config['output_dir'],
            num_train_epochs=training_config['num_train_epochs'],
            per_device_train_batch_size=training_config['per_device_train_batch_size'],
            per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
            gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
            learning_rate=float(training_config['learning_rate']),
            weight_decay=float(training_config['weight_decay']),
            warmup_ratio=training_config.get('warmup_ratio', 0.1),
            max_grad_norm=float(training_config.get('max_grad_norm', 1.0)),
            
            # ä¿å­˜å’Œè¯„ä¼°
            save_steps=training_config.get('save_steps', 100),
            eval_steps=training_config.get('eval_steps', 50),
            logging_steps=training_config.get('logging_steps', 10),
            eval_strategy=training_config.get('evaluation_strategy', 'steps'),
            save_strategy=training_config.get('save_strategy', 'steps'),
            
            # è®¾å¤‡ç›¸å…³ä¼˜åŒ–é…ç½®
            fp16=training_config.get('fp16', False) and not is_mps_device,  # MPSä¸æ”¯æŒfp16
            bf16=training_config.get('bf16', False),
            dataloader_num_workers=dataloader_num_workers,
            dataloader_pin_memory=dataloader_pin_memory,
            seed=self.config['system'].get('seed', 42),
            
            # æ—¥å¿—å’ŒæŠ¥å‘Š
            logging_dir=f"{training_config['output_dir']}/logs",
            report_to=training_config.get('report_to', []),
            
            # æœ€ä½³æ¨¡å‹ä¿å­˜
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=3,
            
            # ç§»é™¤æœªä½¿ç”¨çš„åˆ—
            remove_unused_columns=False,
        )
        
        # å‡†å¤‡æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = self.prepare_datasets()
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
            pad_to_multiple_of=8 if training_args.fp16 else None
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        # PEFTæ¨¡å‹çš„label_namesè­¦å‘Šæ˜¯ä¿¡æ¯æ€§çš„ï¼Œä¸å½±å“è®­ç»ƒ
        # Trainerä¼šè‡ªåŠ¨ä»æ•°æ®é›†ä¸­æ£€æµ‹labelsåˆ—
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        logger.info("è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
        
        return training_args
    
    def train(self):
        """
        å¼€å§‹è®­ç»ƒ
        """
        logger.info("=== å¼€å§‹ç”„å¬›è§’è‰²æ¨¡å‹è®­ç»ƒ ===")
        
        try:
            # å¼€å§‹è®­ç»ƒ
            logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            train_result = self.trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            logger.info("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            self.trainer.save_model()
            self.tokenizer.save_pretrained(self.config['training']['output_dir'])
            
            logger.info("=== è®­ç»ƒå®Œæˆ ===")
            logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {self.config['training']['output_dir']}")
            
            return train_result
            
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œç”„å¬›è§’è‰²æ¨¡å‹è®­ç»ƒ
    """
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        trainer = HuanHuanTrainer()
        
        # è®­ç»ƒæ¨¡å‹
        train_result = trainer.train()
        
        logger.info("ğŸ‰ ç”„å¬›è§’è‰²æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        logger.info("ğŸ“ æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨ Ollama éƒ¨ç½²æ¨¡å‹: ollama create huanhuan -f deployment/Modelfile.huanhuan")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()